{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Python and Numpy\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "**Why Python?** \n",
    "\n",
    "Python is a popular choice in various fields, including computer vision, due to its simplicity and readability, making it great for beginners. It also has a strong community support and a wealth of libraries, which makes complex tasks more manageable. This combination of ease of use and powerful features makes Python a go-to language for both learning and implementing advanced concepts.\n",
    "\n",
    "\n",
    "**Yupiter Notebook**\n",
    "\n",
    "Jupyter Notebooks are interactive coding environments in a webpage format. Write your code between the ### START CODE HERE ### and ### END CODE HERE ### comments. To run your code, press \"SHIFT\"+\"ENTER\" or click the \"Run Cell\" button with a play symbol. Don't worry about the exact number of lines of code; \"(≈ X lines of code)\" is just a guide. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Getting started with Numpy\n",
    "**What is Numpy?**\n",
    "\n",
    "Numpy is a fundamental package in Python used for scientific computing. It allows us to work with high-performance arrays and matrices, making operations that involve large amounts of data much faster and easier. Numpy is not just useful in computer vision; it's a versatile tool that's essential in various areas of data analysis, scientific research, and engineering. Its efficiency and wide range of functionalities make it a cornerstone for anyone working with data in Python.\n",
    "If you want to learn more about Numpy, visit https://Numpy.org/.\n",
    "\n",
    "### 1.1 - Applying function to arrays\n",
    "\n",
    "We will start with the sigmoid function. $sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the sigmoid function, we need the exponential function. Numpy lets you apply an arithmetic operation or a function to every element of an array $ x = (x_1, x_2, ..., x_n)$ (row vector). Try it our yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Array (row vector): [1 2 3 4 5]\n",
      "Array plus 10: [11 12 13 14 15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # this means you can access Numpy functions by writing np.function() instead of Numpy.function()\n",
    "\n",
    "# Create a one-dimensional Numpy array\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "print(\"1D Array (row vector):\", x)\n",
    "\n",
    "# Basic arithmetic operations, the output is the same shape as the input\n",
    "x_plus_10 = x + 10\n",
    "print(\"Array plus 10:\", x_plus_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the exponential function np.exp() to every element in the original array $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^x: [  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591 ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "x = np.exp(x)\n",
    "### END CODE HERE ###\n",
    "print(\"e^x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "e^x: [  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the sigmoid function using Numpy. \n",
    "\n",
    "**Instructions**: x could now be either a real number, a vector, or a matrix. The data structures we use in Numpy to represent these shapes (vectors, matrices...) are called Numpy arrays. You don't need to know more for now.\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or Numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "As you've seen in lecture, you will need to compute gradients to optimize loss functions using backpropagation. Let's code your first gradient function.\n",
    "\n",
    "**Exercise**: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. Use the formular from the lecture (is is very simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or Numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    ds = sigmoid(x) * (1 - sigmoid(x))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<td> [ 0.19661193  0.10499359  0.04517666] </td>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Two common Numpy functions used in deep learning are [np.shape](https://docs.scipy.org/doc/Numpy/reference/generated/Numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/Numpy/reference/generated/Numpy.reshape.html). \n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape(...) is used to reshape X into some other dimension. \n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). Using the reshape method $ndarray.reshape(shape)$.\n",
    "\n",
    "- Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need with `image.shape[0]`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a Numpy array of shape (length, height, depth)\n",
    "\n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = np.reshape(image, (image.shape[0] * image.shape[1] * image.shape[2], 1))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array(\n",
    "         [[[ 0.67826139,  0.29380381], [ 0.90714982,  0.52835647], [ 0.4215251 ,  0.45017551]],\n",
    "         [[ 0.92814219,  0.96677647], [ 0.85304703,  0.52351845], [ 0.19981397,  0.27417313]],\n",
    "         [[ 0.60659855,  0.00533165], [ 0.10820313,  0.49978937], [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print(image2vector(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<td>\n",
    "[[0.67826139]\n",
    " [0.29380381]\n",
    " [0.90714982]\n",
    " [0.52835647]\n",
    " [0.4215251 ]\n",
    " [0.45017551]\n",
    " [0.92814219]\n",
    " [0.96677647]\n",
    " [0.85304703]\n",
    " [0.52351845]\n",
    " [0.19981397]\n",
    " [0.27417313]\n",
    " [0.60659855]\n",
    " [0.00533165]\n",
    " [0.10820313]\n",
    " [0.49978937]\n",
    " [0.34144279]\n",
    " [0.94630077]] </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Centering Input Examples Around Feature Mean\n",
    "In machine learning and deep learning, normalizing data is a common practice, often leading to better performance as it aids in faster convergence of gradient descent algorithms. Normalization in this context refers to adjusting data such that the mean of each feature across the dataset is zero. This is accomplished by subtracting the mean of each feature from the corresponding values.\n",
    "\n",
    "Consider the following example where $x$ is a dataset:\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    1 & 5 & 9 \\\\\n",
    "    4 & 7 & 10 \\\\\n",
    "    3 & 2 & 6 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$\n",
    "In this matrix, the dimensions (n, m) represent three features (n) and three training examples (m). To center the data around the feature mean, we calculate the mean of each row (feature) using Numpy's mean function with axis=1 and keepdims=True:\n",
    "$$\\bar{x} =  x_{mean} = np.mean(x, axis = 1, keepdims=True) = \\begin{bmatrix}    5\\\\\n",
    "    7 \\\\\n",
    "    3.6667 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$\n",
    "Then, we shift the original matrix x by this mean:\n",
    "$$ x_{shifted} = x - x_{mean} = \\begin{bmatrix}\n",
    "   -4 &  0 &  4 \\\\\n",
    "   -3 & 0 & 3\\\\\n",
    " -0.6667  & -1.6667  & 2.3333 \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "Notice how we can subtract matrices of different sizes, thanks to broadcasting in Numpy.\n",
    "\n",
    "**Exercise**: Implement the `shiftRowsAroundMean()` function to normalize the data by centering each feature (row) of a matrix around its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftRowsAroundMean(x):\n",
    "    \"\"\"\n",
    "    Normalize the data by shifting each row of the matrix x around the mean of that row.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A Numpy matrix of shape (n, m), where n is the number of features, and m is the number of examples\n",
    "\n",
    "    Returns:\n",
    "    x_shifted -- The matrix with normalized data.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    x_shifted = x - np.mean(x, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifted Matrix:\n",
      " [[-4.          0.          4.        ]\n",
      " [-3.          0.          3.        ]\n",
      " [-0.66666667 -1.66666667  2.33333333]]\n"
     ]
    }
   ],
   "source": [
    "# Test the function with an example matrix\n",
    "example_matrix = np.array([[1, 5, 9], [4, 7, 10], [3, 2, 6]])\n",
    "shifted_matrix = shiftRowsAroundMean(example_matrix)\n",
    "print(\"Shifted Matrix:\\n\", shifted_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    " [[-4.          0.          4.        ]\n",
    " [-3.          0.          3.        ]\n",
    " [-0.66666667 -1.66666667  2.33333333]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "**What you need to remember:**\n",
    "\n",
    "* The sigmoid function and it's derivative\n",
    "* How to use Numpy to efficiently manipulate arrays\n",
    "* Data normalization and array transformations which are frequently used in machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Vectorization\n",
    "### 2.1 Definition and importance\n",
    "Vectorization is a pivotal concept in the field of data science and machine learning, particularly when it comes to optimizing computational efficiency. At its core, vectorization is about expressing operations as occurring on entire arrays or matrices, rather than through explicit iteration over individual elements.\n",
    "\n",
    "In the context of programming, especially with languages like Python, vectorization refers to the practice of using more abstract and powerful operations that act on whole arrays or datasets at once, rather than using slower, explicit loops. This not only results in **cleaner, more readable code** but also takes advantage of underlying optimizations and parallel processing capabilities, leading to **significant speed improvements**.\n",
    "\n",
    "For instance, in Python, libraries such as Numpy are built with vectorization in mind, allowing for operations on entire arrays with a single line of code. \n",
    "\n",
    "Let's take an example where we will sum the elements of a large array. We'll compare a traditional loop-based approach with a vectorized approach using Numpy, and measure the execution time for both. To create a large array, we will use *np.random.rand* which generates a large array of random floating-point numbers, where each number is uniformly distributed between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum using loop: 499791.979\n",
      "Time taken by loop: 0.14855 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Creating a large array\n",
    "large_array = np.random.rand(1000000)\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Summing using a loop\n",
    "sum_loop = 0\n",
    "for i in large_array:\n",
    "    sum_loop += i\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "time_loop = end_time - start_time\n",
    "\n",
    "print(f\"Sum using loop: {sum_loop:.3f}\")\n",
    "print(f\"Time taken by loop: {time_loop:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum using loop: 499791.979\n",
      "Time taken by loop: 0.00200 seconds\n",
      "Speed-up: ~74x\n"
     ]
    }
   ],
   "source": [
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Summing using Numpy's sum function\n",
    "sum_vectorized = np.sum(large_array)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "time_vectorized = end_time - start_time\n",
    "\n",
    "print(f\"Sum using loop: {sum_vectorized:.3f}\")\n",
    "print(f\"Time taken by loop: {time_vectorized:.5f} seconds\")\n",
    "\n",
    "print(f\"Speed-up: ~{time_loop/time_vectorized:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementing L2 Loss Function\n",
    "**Remind**: The L2 loss function, also known as the mean squared error or quadratic loss, is commonly used in regression problems. It measures the squared average difference between the actual and predicted values. The formula for L2 loss is:\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\frac{1}{2 m}\\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$\n",
    "where $y$ is the vector of true values, and $\\hat{y}$ is the vector of predicted values.\n",
    "The L2 loss is a method to quantify the difference between the predicted and true values. A lower L2 loss value indicates a better model performance.\n",
    "\n",
    "**Exercise**: Implement the `calculate_l2_loss` function to compute the L2 loss for given true values and predictions. You should use `np.dot` for this task.\n",
    "\n",
    "**Note**:\n",
    "* The `np.dot` function computes the dot product of two arrays. For 1-D arrays, it computes the inner product, and for 2-D arrays, it's equivalent to matrix multiplication. In the context of the L2 loss function, np.dot is used to calculate the sum of the squared differences. \\\n",
    "Example: np.dot(a, b) where a and b are 1-D arrays.\n",
    "\n",
    "* In Numpy, the `*` operator is used for element-wise multiplication of arrays. If you multiply two arrays of the same shape using *, each pair of elements at corresponding positions in the arrays are multiplied together. \\\n",
    "Example: a * b multiplies each element of array a with the corresponding element of array b.\n",
    "\n",
    "* `np.multiply` is another way to perform element-wise multiplication. It's functionally identical to using the `*` operator but is explicitly named, which can be more readable. \\\n",
    "Example: np.multiply(a, b) is equivalent to a * b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l2_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the L2 loss between y_true and y_pred.\n",
    "\n",
    "    Arguments:\n",
    "    - y_true: Numpy array of true values\n",
    "    - y_pred: Numpy array of predicted values\n",
    "\n",
    "    Returns:\n",
    "    - l2_loss: Calculated L2 loss as a float\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "    diff = y_pred - y_true\n",
    "    l2_loss = np.multiply(1/(2*y_true.shape[0]), np.dot(diff, diff))\n",
    "\n",
    "    # oder:\n",
    "    #l2_loss = np.multiply(1/(2*y_true.shape[0]), np.multiply(np.square(y_true, y_pred)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 0.010000000000000018\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "y_true = np.array([1.0, 2.0, 3.0])\n",
    "y_pred = np.array([1.1, 1.9, 3.2])\n",
    "\n",
    "l2_loss = calculate_l2_loss(y_true, y_pred)\n",
    "print(\"L2 Loss:\", l2_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.01 </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this assignment. We hope that this little warm-up exercise helps you in the future assignments, which will be more exciting and interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "What you need to remember about Vectorization:\n",
    "\n",
    "* Vectorization is key for speed and efficiency in Python, particularly with Numpy.\n",
    "* It allows for element-wise operations on arrays without explicit loops.\n",
    "* Utilize Numpy's vectorized functions like np.sum, np.dot for performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used for this exercise:\n",
    "- Respective problem sets in the Deep Learning Specialization from deeplearning.ai (https://www.coursera.org/specializations/deep-learning).\n",
    "- DHBW Student input from Gregori Daiger and Lenz Blattner\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
